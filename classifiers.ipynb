{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Systems\n",
    "\n",
    "### Three different classifiers are implemented to analyze tradeoff between interpretability and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "df = pd.read_pickle('wdbc.pkl')\n",
    "\n",
    "X = df[df.columns[2:]]\n",
    "y = df['malignant']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    df_stats = pd.DataFrame({\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall]\n",
    "    })\n",
    "\n",
    "    df_confusion = pd.DataFrame(confusion, columns=['Predicted Benign', 'Predicted Malignant'], index=['Actual Benign', 'Actual Malignant'])\n",
    "    \n",
    "    print(df_stats)\n",
    "    print(df_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rule-based classifier\n",
    "* If [cell size is abnormal]:\n",
    "* or [cell shape is abnormal]\n",
    "* or [cell texture is abnormal]\n",
    "* or [cell homogeneity is abnormal], \n",
    "* then: diagnosis is malignant, \n",
    "* otherwise: diagnosis is benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedClassifier:\n",
    "    def __init__(self, features):\n",
    "        # Initialize a dictionary, set initial values to 0\n",
    "        self.thresholds = {feature: 0 for feature in features}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # We find the thresholds by finding the mean\n",
    "        # of a malignant sample and a benign sample\n",
    "        # and taking the midpoint as the threshold.\n",
    "        for feature in self.thresholds:\n",
    "            malignant_mean = X[y==1][feature].mean()\n",
    "            benign_mean = X[y==0][feature].mean()\n",
    "            self.thresholds[feature] = (malignant_mean + benign_mean) / 2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Use numpy operations rather than just\n",
    "        # a for loop, as these are more optimized.\n",
    "        over_thresholds = np.array(\n",
    "            [X[feature] > self.thresholds[feature] for feature in self.thresholds.keys()]\n",
    "        )\n",
    "        res = np.any(over_thresholds, axis=0).astype(int)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Precision   Recall\n",
      "0  0.614035   0.488095  0.97619\n",
      "                  Predicted Benign  Predicted Malignant\n",
      "Actual Benign                   29                   43\n",
      "Actual Malignant                 1                   41\n"
     ]
    }
   ],
   "source": [
    "rule_based_classifier = RuleBasedClassifier([\n",
    "    \"radius_2\",\n",
    "    \"concavity_2\",\n",
    "    \"texture_2\",\n",
    "    \"smoothness_2\"\n",
    "])\n",
    "rule_based_classifier.fit(X_train, y_train)\n",
    "evaluate_classifier(rule_based_classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: Proportion of correct predictions\n",
    "Precision: True positives predictions out of all positive predictions. \n",
    "    - Tells how reliable the positive predictions are. High precision: predicted possible outcome is usually correct.\n",
    "Recall: Proportion of actual positive samples that were correctly identified by the model. \n",
    "    - Tells you how well the model detects positive cases. High recall means the model is good at identifying positive samples, but it may also increase false positives. \n",
    "\n",
    "A lot of false positives in the confusion matrix. At least thats better than false negatives in our context (healtcare)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Precision    Recall\n",
      "0   0.95614        1.0  0.880952\n",
      "                  Predicted Benign  Predicted Malignant\n",
      "Actual Benign                   72                    0\n",
      "Actual Malignant                 5                   37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_rf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=1)\n",
    "classifier_rf.fit(X_train, y_train)\n",
    "\n",
    "evaluate_classifier(classifier_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression Classifier:\n",
    "#### Attempts to trade off interpretability and classification performance. Logistic Regression has a probabilistic output which can be used to understand how confident the model is in its predictions, which is one way to define interpretability. The weights of each feature can also be accessed which gives insight into their importance to the outcome. First, logistic regression is implemented from scratch. Then scikit-learns LogisticRegression is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.43805758  0.          0.32823369  0.0064169   0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.02533745  0.          0.          0.          0.\n",
      "   0.          0.         -0.26688716  0.17058242  0.07087087  0.02301815\n",
      "   0.          0.          3.15178865  0.          0.          0.        ]]\n",
      "   Accuracy  Precision    Recall\n",
      "0  0.964912        1.0  0.904762\n",
      "                  Predicted Benign  Predicted Malignant\n",
      "Actual Benign                   72                    0\n",
      "Actual Malignant                 4                   38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression(solver='liblinear', penalty='l1', ) # Using L2 regularization\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "print(lr_classifier.coef_)\n",
    "\n",
    "evaluate_classifier(lr_classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitRegression:\n",
    "    def __init__(self, learning_rate=1e-1, max_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        self.weights = np.zeros(self.n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # compute predicted values (logistc regression hypothesis)\n",
    "            A = 1 / (1 + np.exp(-(self.X.dot(self.weights) + self.b)))\n",
    "\n",
    "            # compute gradients\n",
    "            tmp = A - self.y # Difference between predicted and actual values\n",
    "            dW = np.dot(self.X.T, tmp) / self.n_samples\n",
    "            db = np.sum(tmp) / self.n_samples\n",
    "\n",
    "            # update weights\n",
    "            self.weights = self.weights - self.learning_rate * dW\n",
    "            self.b = self.b - self.learning_rate * db\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        A = 1 / (1 + np.exp(- (X.dot(self.weights) + self.b)))\n",
    "        predicted_labels = np.array([1 if p >= 0.5 else 0 for p in A])\n",
    "        return predicted_labels\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        A = 1 / (1 + np.exp(-(X.dot(self.weights) + self.b)))\n",
    "        return A \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.94838314e-02 -4.13824581e-02 -1.69573302e-01 -6.36108821e-02\n",
      " -2.75758091e-04  1.62055426e-04  5.86971411e-04  2.40257976e-04\n",
      " -5.24297096e-04 -2.16288490e-04 -2.48958857e-04 -2.66630485e-03\n",
      "  5.53464490e-04  6.93350815e-02 -1.40665172e-05  5.48151057e-05\n",
      "  9.38729510e-05  1.27095540e-05 -4.07096427e-05 -1.61594625e-06\n",
      " -3.15699909e-02 -5.20735136e-02 -1.70565440e-01  9.34413571e-02\n",
      " -3.53981875e-04  6.49020106e-04  1.27152414e-03  2.88932726e-04\n",
      " -7.20633766e-04 -1.99521545e-04]\n",
      "   Accuracy  Precision    Recall\n",
      "0  0.938596   0.948718  0.880952\n",
      "                  Predicted Benign  Predicted Malignant\n",
      "Actual Benign                   70                    2\n",
      "Actual Malignant                 5                   37\n"
     ]
    }
   ],
   "source": [
    "columns = [column for column in X.columns if \"_2\" in column]\n",
    "\n",
    "lr2_classifier = LogitRegression(learning_rate=1e-4, max_iter=1000)\n",
    "lr2_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(lr2_classifier.weights)\n",
    "evaluate_classifier(lr2_classifier, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
